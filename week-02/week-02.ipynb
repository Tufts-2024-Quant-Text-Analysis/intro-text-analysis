{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "\n",
    "## Resources\n",
    "\n",
    "- https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "- https://www.opengreekandlatin.org/what-is-a-cts-urn/\n",
    "- https://cite-architecture.github.io/xcite/ctsurn-quick/\n",
    "\n",
    "## Catch up and review\n",
    "\n",
    "### Reading a file into memory\n",
    "\n",
    "Can you read one of the files from last week into memory? Enter the code to do so below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for reading a file goes here.\n",
    "with open('../week-01/austen-pride-and-prejudice.txt') as f:\n",
    "    austen = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Git forking, branching, and pushing\n",
    "\n",
    "In this section, we're going to talk about git forking, branching, and pushing, as this will be the main way that you'll submit homework.\n",
    "\n",
    "First, you'll want to navigate to the GitHub repository for this course (https://github.com/Tufts-2024-Quant-Text-Analysis/intro-text-analysis) and press the \"Fork\" button:\n",
    "\n",
    "![Screenshot of the Fork button](./img/fork.png)\n",
    "\n",
    "You should then see a menu that looks something like this:\n",
    "\n",
    "![Screenshot of Fork menu](./img/fork-menu.png)\n",
    "\n",
    "You can rename the repository if you wish, just make sure to keep track of what you rename it to!\n",
    "\n",
    "Once you have forked the main repository, go to your fork and click the \"Code\" button:\n",
    "\n",
    "![Screenshot of the Code button](./img/code.png)\n",
    "\n",
    "You can then clone your fork by copying the URL from the dropdown and entering the following in your terminal:\n",
    "\n",
    "```sh\n",
    "git clone YOUR_GIT_URL_HERE\n",
    "```\n",
    "\n",
    "### Setting up an upstream\n",
    "\n",
    "By default, your own fork of the repository will be the `origin` for this clone. It is a convention when working with git forks to call the \"main\" repository `upstream`. You can add `upstream` as a remote by running the following from within your clone:\n",
    "\n",
    "```sh\n",
    "git remote add upstream https://github.com/Tufts-2024-Quant-Text-Analysis/intro-text-analysis.git\n",
    "```\n",
    "\n",
    "If you know run `git remote -v` from that directory, you should see both `origin` and `upstream`.\n",
    "\n",
    "**NEVER** push directly to `upstream`. Instead, **`pull`** from `upstream` and **`push`** to your fork.\n",
    "\n",
    "Whenever you push your work to your fork, you can navigate to it (on the web) and see an option to create a Pull Request:\n",
    "\n",
    "![Screenshot of pull request](./img/pull-request.png)\n",
    "\n",
    "Try it now: create a small change (you can just add one of your answers to Week 01), and push it to your fork.\n",
    "\n",
    "I will create a branch that matches each of your usernames on the main repository. Open your pull request against this branch.\n",
    "\n",
    "Now we have an easy way of looking at the changes that you've made and comparing them to the main repository without clobbering each other's work.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Data \n",
    "\n",
    "### Discuss \n",
    "\n",
    "- What kind(s) of visualization would be best for showing the relative frequencies of a verb like καλός in the Platonic corpus versus Thucydides?\n",
    "\n",
    "Refer to @Brezina2018 [ch. 1] if you feel stuck.\n",
    "\n",
    "## Installing packages\n",
    "\n",
    "Inside a Jupyter/Colab notebook (they're functionally the same thing), you can install packages with the magic command `%pip`. See [here](https://ipython.readthedocs.io/en/stable/interactive/magics.html) for more info on iPython/Jupyter Notebook magic commands.\n",
    "\n",
    "We're going to need the `lxml` package in a moment, so let's go ahead and install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (5.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CTS URNs\n",
    "\n",
    "Before we go further, we're going to need to talk a bit about Canonical Text Services Universal Resource Names -- or **CTS URN**s for short.\n",
    "\n",
    "### Collection\n",
    "\n",
    "CTS URNs allow us to specify text down to the token level. They work as references to specific components of a larger corpus, starting with the **collection**.\n",
    "\n",
    "```\n",
    "urn:cts:greekLit\n",
    "```\n",
    "\n",
    "The prefix `urn:cts:` is required by the protocol; `greekLit` refers to the collection of Greek texts known to the CTS implementation.\n",
    "\n",
    "### Work Component\n",
    "\n",
    "The next element in a CTS URN is collectively referred to as the **work component**. At a minimum, it contains a reference to a **text group**. \n",
    "\n",
    "#### Text Group\n",
    "\n",
    "Text groups are often what we think of as authors, but by treating them as placeholders not for a specific writer but for canonically related texts, we can stay one step ahead of issues about attribution etc. Text groups are not meant to make any assertions about authorship; they're just a convenient way to find things.\n",
    "\n",
    "For example, the _Rhesus_ is contained within the Euripides text group by convention; we aren't weighing in on that vexed authorship question.\n",
    "\n",
    "```\n",
    "urn:cts:greekLit:tlg0525\n",
    "```\n",
    "\n",
    "`tlg0525` refers to Pausanias. You can use https://cts.perseids.org/ to look up URNs, but since we'll be working a lot with Pausanias' _Periegesis_, it might be a good idea to get used to tlg0525.\n",
    "\n",
    "Why `tlg0525` and not just `pausanias`? Names and their orthography are hard to standardize. CTS URNs are designed to be **universal** and portable. Using names as identifiers too early on would lead to unnecessary confusion.\n",
    "\n",
    "Should we have settled on a system other than the numbering that the TLG came up with? Probably, but we're decades too late to change that now.\n",
    "\n",
    "\n",
    "#### Work\n",
    "\n",
    "Next comes the **work**. This refers to the item -- in our case it will usually be a text -- under the text group.\n",
    "\n",
    "```\n",
    "urn:cts:greekLit:tlg0525.tlg001\n",
    "```\n",
    "\n",
    "Notice that `tlg001` is separated from `tlg0525` by a `.`, rather than a `:`. This is because only major components of the URN are separated by `:`; minor components, such as the sub-components of the major **work component**, are separated by `.`.\n",
    "\n",
    "Works within the work component are usually numbered sequentially for the items that we're dealing with. For Sophocles, the sequence starts with _Trachiniae_, so `urn:cts:greekLit:tlg0011.tlg001` refers to that text; `urn:cts:greekLit:tlg0011.tlg003`, for example, refers to _Ajax_.\n",
    "\n",
    "#### Version\n",
    "\n",
    "For classical texts, which have any number of editions published over the years, the **version** is essential. It helps us point to a specific edition of the work, complete with that editions editorial interventions.\n",
    "\n",
    "```\n",
    "urn:cts:greekLit:tlg0525.tlg001.perseus-grc2\n",
    "```\n",
    "\n",
    "The `perseus-grc2` version refers to the second Greek edition of the _Periegesis_ as published by the Perseus Digital Library.\n",
    "\n",
    "#### Exemplar\n",
    "\n",
    "There is another element in the work component of CTS URNs, the **exemplar**. You might think of this as a reference to the specific _witness_ that you're dealing with.\n",
    "\n",
    "We won't need to use this much for retrieving texts, but if you're working on different ways of handling textual material, you might want to append an exemplar fragment to the work component.\n",
    "\n",
    "For example, if I've made additional annotations to the Perseus _Periegesis_ for my own research that don't necessarily belong in the canonical version (via a pull request vel sim.), I might dub my local exemplar:\n",
    "\n",
    "```\n",
    "urn:cts:greekLit:tlg0525.tlg001.perseus-grc2.charles-annotations\n",
    "```\n",
    "\n",
    "That way I know that this URN refers to an exemplar that contains annotations that might not be present in the parent `perseus-grc2`.\n",
    "\n",
    "I want to emphasize, again, you can get through this course just fine without ever using an exemplar fragment. They're under-specified and confusing, but I mention them here for the sake of completeness.\n",
    "\n",
    "### Passage Component\n",
    "\n",
    "Finally, CTS URNs can have a **passage component**. This is the most specific part of the CTS URN, containing references to precise passages and even words within a text.\n",
    "\n",
    "```\n",
    "urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1\n",
    "```\n",
    "\n",
    "The `1` above refers to Pausanias Book 1.\n",
    "\n",
    "```\n",
    "urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1.1\n",
    "```\n",
    "\n",
    "Now it references Book 1, Chapter 1.\n",
    "\n",
    "```\n",
    "urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1.1-2.2\n",
    "```\n",
    "\n",
    "Now we're talking about a passage spanning Book 1, Chapter 1, to Book 2, Chapter 2.\n",
    "\n",
    "```\n",
    "urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1.1.5@Κωλιάς\n",
    "```\n",
    "\n",
    "Now we're referencing the token `Κωλιάς` in Book 1, Chapter 1, Section 5.\n",
    "\n",
    "```\n",
    "urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1.1.5@Κωλιάδος-1.1.5@θεαί\n",
    "```\n",
    "\n",
    "As a final example, this URN references the span from Κωλιάδος to θεαί in Book 1, Chapter 1, Section 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting text to work with\n",
    "\n",
    "So why this detour on CTS URNs? Because it will make it easier for you to find the texts that you need. I've already added the perseus-grc2 version of Pausanias to this repository; you can find it under `tei/tlg0525.tlg001.perseus-grc2.xml`. (The URN is abbreviated because the file comes from the [PerseusDL/canonical-greekLit](https://github.com/PerseusDL/canonical-greekLit/) repo on GitHub.)\n",
    "\n",
    "Ideally, we would be able to request these texts from an API, but as of this writing in August 2024, all of the known APIs are not working. So for now, we will parse these files locally and transform them into data structures that facilitate our analyses.\n",
    "\n",
    "We'll first need to install the MyCapytains library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: MyCapytain in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (3.0.2)\n",
      "Requirement already satisfied: requests>=2.8.1 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from MyCapytain) (2.32.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from MyCapytain) (1.16.0)\n",
      "Requirement already satisfied: lxml>=3.6.4 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from MyCapytain) (5.3.0)\n",
      "Requirement already satisfied: future>=0.16.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from MyCapytain) (1.0.0)\n",
      "Requirement already satisfied: rdflib-jsonld>=0.4.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from MyCapytain) (0.6.2)\n",
      "Requirement already satisfied: LinkHeader>=0.4.3 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from MyCapytain) (0.4.3)\n",
      "Requirement already satisfied: pyld>=1.0.3 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from MyCapytain) (2.0.4)\n",
      "Requirement already satisfied: typing in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from MyCapytain) (3.7.4.3)\n",
      "Requirement already satisfied: cachetools in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from pyld>=1.0.3->MyCapytain) (5.5.0)\n",
      "Requirement already satisfied: frozendict in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from pyld>=1.0.3->MyCapytain) (2.4.4)\n",
      "Requirement already satisfied: rdflib>=5.0.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from rdflib-jsonld>=0.4.0->MyCapytain) (7.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from requests>=2.8.1->MyCapytain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from requests>=2.8.1->MyCapytain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from requests>=2.8.1->MyCapytain) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from requests>=2.8.1->MyCapytain) (2024.8.30)\n",
      "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from rdflib>=5.0.0->rdflib-jsonld>=0.4.0->MyCapytain) (0.6.1)\n",
      "Requirement already satisfied: pyparsing<4,>=2.1.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from rdflib>=5.0.0->rdflib-jsonld>=0.4.0->MyCapytain) (3.1.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install MyCapytain\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can import this module and use it to ingest the text of Pausanias, stored in the `tei/` directory of this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyCapytain.resources.texts.local.capitains.cts import CapitainsCtsText\n",
    "\n",
    "with open(\"../tei/tlg0525.tlg001.perseus-grc2.xml\") as f:\n",
    "    text = CapitainsCtsText(urn=\"urn:cts:greekLit:tlg0525.tlg001.perseus-grc2\", resource=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try turning the text into just a [Pandas DataFrame](https://pandas.pydata.org/docs/index.html) with columns for the CTS URN, the corresponding XML, and the unannotated text of the passage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block might take a while\n",
    "\n",
    "from lxml import etree\n",
    "from MyCapytain.common.constants import Mimetypes\n",
    "\n",
    "urns = []\n",
    "raw_xmls = []\n",
    "unannotated_strings = []\n",
    "\n",
    "for ref in text.getReffs(level=len(text.citation)):\n",
    "    urn = f\"{text.urn}:{ref}\"\n",
    "    node = text.getTextualNode(ref)\n",
    "    raw_xml = node.export(Mimetypes.XML.TEI)\n",
    "    tree = node.export(Mimetypes.PYTHON.ETREE)\n",
    "    s = etree.tostring(tree, encoding=\"unicode\", method=\"text\")\n",
    "\n",
    "    urns.append(urn)\n",
    "    raw_xmls.append(raw_xml)\n",
    "    unannotated_strings.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "d = {\n",
    "    \"urn\": pd.Series(urns, dtype=\"string\"),\n",
    "    \"raw_xml\": raw_xmls,\n",
    "    \"unannotated_strings\": pd.Series(unannotated_strings, dtype=\"string\")\n",
    "}\n",
    "pausanias_df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with textual data\n",
    "\n",
    "Now that we have some text to work with -- and by \"some text,\" I mean all 3170 sections of Pausanias in the above DataFrame -- we can start working with the data.\n",
    "\n",
    "Before doing so, however, we should ask _how_ we're going to make the data more manageable -- it isn't exactly feasible to dive headfirst into a corpus of this size.\n",
    "\n",
    "> Discuss: What units can we break Pausanias down into to make it more manageable? Don't worry about how you would do it in code yet, just think about how you might explore the units of the text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of words\n",
    "\n",
    "With all languages, but especially with heavily-inflected languages like ancient Greek and Latin, it is important to be precise about the kinds of word forms that we're dealing with.\n",
    "\n",
    "#### Tokens \n",
    "\n",
    "A **token** or **running word** \"is a single occurrence of a word form in the text\" [@Brezina2018 39].\n",
    "\n",
    "How can we count the number of tokens in all of Pausanias? First we need to **tokenize** the `unannotated_strings` column of `pausanias_df`.\n",
    "\n",
    "Tokenization is a surprisingly complicated process depending on the language of study, and we will learn more sophisticated methods for tokenizing Greek text as we go along.\n",
    "\n",
    "For now, however, let's define a token as \"whitespace-delimited text\" -- we're not going to worry about punctuation etc. just yet.\n",
    "\n",
    "So to tokenize the `unannotated_strings` column of `pausanias_df`, we can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urn</th>\n",
       "      <th>raw_xml</th>\n",
       "      <th>unannotated_strings</th>\n",
       "      <th>tokens</th>\n",
       "      <th>whitespaced_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1...</td>\n",
       "      <td>&lt;TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns...</td>\n",
       "      <td>τῆς ἠπείρου τῆς Ἑλληνικῆς κατὰ νήσους τὰς Κυκλ...</td>\n",
       "      <td>(τῆς, ἠπείρου, τῆς, Ἑλληνικῆς, κατὰ, νήσους, τ...</td>\n",
       "      <td>[τῆς, ἠπείρου, τῆς, Ἑλληνικῆς, κατὰ, νήσους, τ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1...</td>\n",
       "      <td>&lt;TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns...</td>\n",
       "      <td>ὁ δὲ Πειραιεὺς δῆμος μὲν ἦν ἐκ παλαιοῦ, πρότερ...</td>\n",
       "      <td>(ὁ, δὲ, Πειραιεὺς, δῆμος, μὲν, ἦν, ἐκ, παλαιοῦ...</td>\n",
       "      <td>[ὁ, δὲ, Πειραιεὺς, δῆμος, μὲν, ἦν, ἐκ, παλαιοῦ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1...</td>\n",
       "      <td>&lt;TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns...</td>\n",
       "      <td>θέας δὲ ἄξιον τῶν ἐν Πειραιεῖ μάλιστα Ἀθηνᾶς ἐ...</td>\n",
       "      <td>(θέας, δὲ, ἄξιον, τῶν, ἐν, Πειραιεῖ, μάλιστα, ...</td>\n",
       "      <td>[θέας, δὲ, ἄξιον, τῶν, ἐν, Πειραιεῖ, μάλιστα, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1...</td>\n",
       "      <td>&lt;TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns...</td>\n",
       "      <td>ἔστι δὲ καὶ ἄλλος Ἀθηναίοις ὁ μὲν ἐπὶ Μουνυχίᾳ...</td>\n",
       "      <td>(ἔστι, δὲ, καὶ, ἄλλος, Ἀθηναίοις, ὁ, μὲν, ἐπὶ,...</td>\n",
       "      <td>[ἔστι, δὲ, καὶ, ἄλλος, Ἀθηναίοις, ὁ, μὲν, ἐπὶ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1...</td>\n",
       "      <td>&lt;TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns...</td>\n",
       "      <td>ἀπέχει δὲ σταδίους εἴκοσιν ἄκρα Κωλιάς· ἐς ταύ...</td>\n",
       "      <td>(ἀπέχει, δὲ, σταδίους, εἴκοσιν, ἄκρα, Κωλιάς, ...</td>\n",
       "      <td>[ἀπέχει, δὲ, σταδίους, εἴκοσιν, ἄκρα, Κωλιάς·,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3165</th>\n",
       "      <td>urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1...</td>\n",
       "      <td>&lt;TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns...</td>\n",
       "      <td>οὗτοι μὲν δὴ ὑπεροικοῦσιν Ἀμφίσσης· ἐπὶ θαλάσσ...</td>\n",
       "      <td>(οὗτοι, μὲν, δὴ, ὑπεροικοῦσιν, Ἀμφίσσης, ·, ἐπ...</td>\n",
       "      <td>[οὗτοι, μὲν, δὴ, ὑπεροικοῦσιν, Ἀμφίσσης·, ἐπὶ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3166</th>\n",
       "      <td>urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1...</td>\n",
       "      <td>&lt;TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns...</td>\n",
       "      <td>κληθῆναι δὲ ἀπὸ γυναικὸς ἢ νύμφης τεκμαίρομαι ...</td>\n",
       "      <td>(κληθῆναι, δὲ, ἀπὸ, γυναικὸς, ἢ, νύμφης, τεκμα...</td>\n",
       "      <td>[κληθῆναι, δὲ, ἀπὸ, γυναικὸς, ἢ, νύμφης, τεκμα...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3167</th>\n",
       "      <td>urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1...</td>\n",
       "      <td>&lt;TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns...</td>\n",
       "      <td>τὰ δὲ ἔπη τὰ Ναυπάκτια ὀνομαζόμενα ὑπὸ Ἑλλήνων...</td>\n",
       "      <td>(τὰ, δὲ, ἔπη, τὰ, Ναυπάκτια, ὀνομαζόμενα, ὑπὸ,...</td>\n",
       "      <td>[τὰ, δὲ, ἔπη, τὰ, Ναυπάκτια, ὀνομαζόμενα, ὑπὸ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3168</th>\n",
       "      <td>urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1...</td>\n",
       "      <td>&lt;TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns...</td>\n",
       "      <td>ἐνταῦθα ἔστι μὲν ἐπὶ θαλάσσῃ ναὸς Ποσειδῶνος κ...</td>\n",
       "      <td>(ἐνταῦθα, ἔστι, μὲν, ἐπὶ, θαλάσσῃ, ναὸς, Ποσει...</td>\n",
       "      <td>[ἐνταῦθα, ἔστι, μὲν, ἐπὶ, θαλάσσῃ, ναὸς, Ποσει...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3169</th>\n",
       "      <td>urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1...</td>\n",
       "      <td>&lt;TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns...</td>\n",
       "      <td>τοῦ δὲ Ἀσκληπιοῦ τὸ\n",
       "ἱερὸν ἐρείπια ἦν, ἐξ ἀρχῆς...</td>\n",
       "      <td>(τοῦ, δὲ, Ἀσκληπιοῦ, τὸ, \\n, ἱερὸν, ἐρείπια, ἦ...</td>\n",
       "      <td>[τοῦ, δὲ, Ἀσκληπιοῦ, τὸ, ἱερὸν, ἐρείπια, ἦν,, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3170 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    urn  \\\n",
       "0     urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1...   \n",
       "1     urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1...   \n",
       "2     urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1...   \n",
       "3     urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1...   \n",
       "4     urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1...   \n",
       "...                                                 ...   \n",
       "3165  urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1...   \n",
       "3166  urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1...   \n",
       "3167  urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1...   \n",
       "3168  urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1...   \n",
       "3169  urn:cts:greekLit:tlg0525.tlg001.perseus-grc2:1...   \n",
       "\n",
       "                                                raw_xml  \\\n",
       "0     <TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns...   \n",
       "1     <TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns...   \n",
       "2     <TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns...   \n",
       "3     <TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns...   \n",
       "4     <TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns...   \n",
       "...                                                 ...   \n",
       "3165  <TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns...   \n",
       "3166  <TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns...   \n",
       "3167  <TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns...   \n",
       "3168  <TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns...   \n",
       "3169  <TEI xmlns=\"http://www.tei-c.org/ns/1.0\" xmlns...   \n",
       "\n",
       "                                    unannotated_strings  \\\n",
       "0     τῆς ἠπείρου τῆς Ἑλληνικῆς κατὰ νήσους τὰς Κυκλ...   \n",
       "1     ὁ δὲ Πειραιεὺς δῆμος μὲν ἦν ἐκ παλαιοῦ, πρότερ...   \n",
       "2     θέας δὲ ἄξιον τῶν ἐν Πειραιεῖ μάλιστα Ἀθηνᾶς ἐ...   \n",
       "3     ἔστι δὲ καὶ ἄλλος Ἀθηναίοις ὁ μὲν ἐπὶ Μουνυχίᾳ...   \n",
       "4     ἀπέχει δὲ σταδίους εἴκοσιν ἄκρα Κωλιάς· ἐς ταύ...   \n",
       "...                                                 ...   \n",
       "3165  οὗτοι μὲν δὴ ὑπεροικοῦσιν Ἀμφίσσης· ἐπὶ θαλάσσ...   \n",
       "3166  κληθῆναι δὲ ἀπὸ γυναικὸς ἢ νύμφης τεκμαίρομαι ...   \n",
       "3167  τὰ δὲ ἔπη τὰ Ναυπάκτια ὀνομαζόμενα ὑπὸ Ἑλλήνων...   \n",
       "3168  ἐνταῦθα ἔστι μὲν ἐπὶ θαλάσσῃ ναὸς Ποσειδῶνος κ...   \n",
       "3169  τοῦ δὲ Ἀσκληπιοῦ τὸ\n",
       "ἱερὸν ἐρείπια ἦν, ἐξ ἀρχῆς...   \n",
       "\n",
       "                                                 tokens  \\\n",
       "0     (τῆς, ἠπείρου, τῆς, Ἑλληνικῆς, κατὰ, νήσους, τ...   \n",
       "1     (ὁ, δὲ, Πειραιεὺς, δῆμος, μὲν, ἦν, ἐκ, παλαιοῦ...   \n",
       "2     (θέας, δὲ, ἄξιον, τῶν, ἐν, Πειραιεῖ, μάλιστα, ...   \n",
       "3     (ἔστι, δὲ, καὶ, ἄλλος, Ἀθηναίοις, ὁ, μὲν, ἐπὶ,...   \n",
       "4     (ἀπέχει, δὲ, σταδίους, εἴκοσιν, ἄκρα, Κωλιάς, ...   \n",
       "...                                                 ...   \n",
       "3165  (οὗτοι, μὲν, δὴ, ὑπεροικοῦσιν, Ἀμφίσσης, ·, ἐπ...   \n",
       "3166  (κληθῆναι, δὲ, ἀπὸ, γυναικὸς, ἢ, νύμφης, τεκμα...   \n",
       "3167  (τὰ, δὲ, ἔπη, τὰ, Ναυπάκτια, ὀνομαζόμενα, ὑπὸ,...   \n",
       "3168  (ἐνταῦθα, ἔστι, μὲν, ἐπὶ, θαλάσσῃ, ναὸς, Ποσει...   \n",
       "3169  (τοῦ, δὲ, Ἀσκληπιοῦ, τὸ, \\n, ἱερὸν, ἐρείπια, ἦ...   \n",
       "\n",
       "                                     whitespaced_tokens  \n",
       "0     [τῆς, ἠπείρου, τῆς, Ἑλληνικῆς, κατὰ, νήσους, τ...  \n",
       "1     [ὁ, δὲ, Πειραιεὺς, δῆμος, μὲν, ἦν, ἐκ, παλαιοῦ...  \n",
       "2     [θέας, δὲ, ἄξιον, τῶν, ἐν, Πειραιεῖ, μάλιστα, ...  \n",
       "3     [ἔστι, δὲ, καὶ, ἄλλος, Ἀθηναίοις, ὁ, μὲν, ἐπὶ,...  \n",
       "4     [ἀπέχει, δὲ, σταδίους, εἴκοσιν, ἄκρα, Κωλιάς·,...  \n",
       "...                                                 ...  \n",
       "3165  [οὗτοι, μὲν, δὴ, ὑπεροικοῦσιν, Ἀμφίσσης·, ἐπὶ,...  \n",
       "3166  [κληθῆναι, δὲ, ἀπὸ, γυναικὸς, ἢ, νύμφης, τεκμα...  \n",
       "3167  [τὰ, δὲ, ἔπη, τὰ, Ναυπάκτια, ὀνομαζόμενα, ὑπὸ,...  \n",
       "3168  [ἐνταῦθα, ἔστι, μὲν, ἐπὶ, θαλάσσῃ, ναὸς, Ποσει...  \n",
       "3169  [τοῦ, δὲ, Ἀσκληπιοῦ, τὸ, ἱερὸν, ἐρείπια, ἦν,, ...  \n",
       "\n",
       "[3170 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html for\n",
    "# panda's string-splitting utilities; it splits on whitespace by default\n",
    "pausanias_df['whitespaced_tokens'] = pausanias_df['unannotated_strings'].str.split()\n",
    "\n",
    "pausanias_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now that we have some arrays of tokens in `whitespaced_tokens` column, how do we count them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'whitespaced_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'whitespaced_tokens'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;28mlen\u001b[39m(ts) \u001b[38;5;28;01mfor\u001b[39;00m ts \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpausanias_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhitespaced_tokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "File \u001b[0;32m/workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'whitespaced_tokens'"
     ]
    }
   ],
   "source": [
    "sum(len(ts) for ts in pausanias_df['whitespaced_tokens'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Discuss: What does the above line of code do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line of code is not very idiomatic for Pandas, however. Instead, we should write something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217416"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pausanias_df['whitespaced_tokens'].explode().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types\n",
    "\n",
    "A **type** is a unique word form in the corpus. For example, the inflected forms βουλεύεται and βουλεύομεν are each a type. (See @Brezina2018 [39-40].)\n",
    "\n",
    "In other words, **types** are **tokens** grouped by form. So to count the number of **types** in Pausanias, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41363"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pausanias_df['whitespaced_tokens'].explode().unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Discuss: Break the above line of code down method by method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to see the top `n` types in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'whitespaced_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'whitespaced_tokens'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[0;32m----> 3\u001b[0m type_counts \u001b[38;5;241m=\u001b[39m Counter(\u001b[43mpausanias_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhitespaced_tokens\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mexplode())\n\u001b[1;32m      5\u001b[0m type_counts\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m/workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'whitespaced_tokens'"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "type_counts = Counter(pausanias_df['whitespaced_tokens'].explode())\n",
    "\n",
    "type_counts.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words\n",
    "\n",
    "Hm, that's not particularly interesting -- most of these words are fairly common and will rank highly in almost any corpus. Further, since we haven't accounted for punctuation, we're probably generating frequencies incorrectly based on whether or not a type is joined to any punctuation. We need to get a bit more sophisticated.\n",
    "\n",
    "Let's install `spacy` and `grecy` to perform better tokenization and incorporate the notion of a **stop word**: a token that is so common that including it in most statistical analyses will just generate noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip==24.0\n",
      "  Using cached pip-24.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Using cached pip-24.0-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.2\n",
      "    Uninstalling pip-24.2:\n",
      "      Successfully uninstalled pip-24.2\n",
      "Successfully installed pip-24.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Using cached typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0 (from spacy)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting requests<3.0.0,>=2.13.0 (from spacy)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting jinja2 (from spacy)\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: setuptools in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Downloading numpy-2.1.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting click>=8.0.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached rich-13.8.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached cloudpathlib-0.19.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached smart_open-7.0.4-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
      "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.7 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading spacy-3.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (46 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.3/46.3 kB\u001b[0m \u001b[31m631.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.10-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (29 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Downloading pydantic_core-2.23.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.9/490.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.2.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (920 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m920.2/920.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Using cached typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-0.7.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.3/167.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.3/140.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached cloudpathlib-0.19.0-py3-none-any.whl (49 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
      "Using cached rich-13.8.1-py3-none-any.whl (241 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached smart_open-7.0.4-py3-none-any.whl (61 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.3/126.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading marisa_trie-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading wrapt-1.16.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.7/80.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, urllib3, tqdm, spacy-loggers, spacy-legacy, shellingham, pydantic-core, numpy, murmurhash, mdurl, MarkupSafe, marisa-trie, idna, cloudpathlib, click, charset-normalizer, certifi, catalogue, annotated-types, srsly, smart-open, requests, pydantic, preshed, markdown-it-py, language-data, jinja2, blis, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "Successfully installed MarkupSafe-2.1.5 annotated-types-0.7.0 blis-0.7.11 catalogue-2.0.10 certifi-2024.8.30 charset-normalizer-3.3.2 click-8.1.7 cloudpathlib-0.19.0 confection-0.1.5 cymem-2.0.8 idna-3.10 jinja2-3.1.4 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.2.0 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.10 numpy-1.26.4 preshed-3.0.9 pydantic-2.9.2 pydantic-core-2.23.4 requests-2.32.3 rich-13.8.1 shellingham-1.5.4 smart-open-7.0.4 spacy-3.7.6 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 tqdm-4.66.5 typer-0.12.5 urllib3-2.2.3 wasabi-1.1.3 weasel-0.4.1 wrapt-1.16.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting grecy\n",
      "  Using cached grecy-1.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: spacy<4.0.0,>=3.5 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from grecy) (3.7.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<4.0.0,>=3.5->grecy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<4.0.0,>=3.5->grecy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<4.0.0,>=3.5->grecy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<4.0.0,>=3.5->grecy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<4.0.0,>=3.5->grecy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<4.0.0,>=3.5->grecy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<4.0.0,>=3.5->grecy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<4.0.0,>=3.5->grecy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<4.0.0,>=3.5->grecy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<4.0.0,>=3.5->grecy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<4.0.0,>=3.5->grecy) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<4.0.0,>=3.5->grecy) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<4.0.0,>=3.5->grecy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<4.0.0,>=3.5->grecy) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<4.0.0,>=3.5->grecy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<4.0.0,>=3.5->grecy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<4.0.0,>=3.5->grecy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<4.0.0,>=3.5->grecy) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<4.0.0,>=3.5->grecy) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.5->grecy) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.5->grecy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.5->grecy) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4.0.0,>=3.5->grecy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5->grecy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5->grecy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5->grecy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.5->grecy) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.5->grecy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<4.0.0,>=3.5->grecy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.5->grecy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.5->grecy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.5->grecy) (13.8.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.5->grecy) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.5->grecy) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from jinja2->spacy<4.0.0,>=3.5->grecy) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4.0.0,>=3.5->grecy) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.5->grecy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.5->grecy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4.0.0,>=3.5->grecy) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4.0.0,>=3.5->grecy) (0.1.2)\n",
      "Using cached grecy-1.0-py3-none-any.whl (6.0 kB)\n",
      "Installing collected packages: grecy\n",
      "Successfully installed grecy-1.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "## Uncomment the line for your system's architecture\n",
    "%pip install pip==24.0\n",
    "%pip install spacy\n",
    "# %pip install 'spacy[apple]'\n",
    "%pip install grecy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to install a model for `grecy` to use. Note that there is a known but so-far unpatched issue where this command will only work with Python 3.11.9 and pip 24.0 (or a bit older in either case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Installing grc_proiel_sm.....\n",
      "\n",
      "Please wait, this could take some minutes.....\n",
      "\n",
      "Collecting grc-proiel-sm==any\n",
      "Downloading https://huggingface.co/Jacobo/grc_proiel_sm/resolve/main/grc_proiel_sm-any-py3-none-any.whl (65.5 MB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/65.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/65.5 MB\u001b[0m \u001b[31m139.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/65.5 MB\u001b[0m \u001b[31m163.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/65.5 MB\u001b[0m \u001b[31m170.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.8/65.5 MB\u001b[0m \u001b[31m163.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.4/65.5 MB\u001b[0m \u001b[31m161.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32.7/65.5 MB\u001b[0m \u001b[31m159.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m37.7/65.5 MB\u001b[0m \u001b[31m148.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/65.5 MB\u001b[0m \u001b[31m169.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m47.9/65.5 MB\u001b[0m \u001b[31m147.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m51.6/65.5 MB\u001b[0m \u001b[31m122.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m55.2/65.5 MB\u001b[0m \u001b[31m104.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m59.1/65.5 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━\u001b[0m \u001b[32m62.9/65.5 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m65.4/65.5 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.5 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from grc-proiel-sm==any) (3.7.6)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (0.12.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (4.66.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (2.9.2)\n",
      "Requirement already satisfied: jinja2 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (1.26.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (2024.8.30)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (13.8.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (0.19.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from jinja2->spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (2.1.5)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /workspaces/intro-text-analysis/.conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.5->grc-proiel-sm==any) (0.1.2)\n",
      "Installing collected packages: grc-proiel-sm\n",
      "Successfully installed grc-proiel-sm-3.7.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%run -m grecy install grc_proiel_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"grc_proiel_sm\", disable=[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nlp.tokenizer\n",
    "\n",
    "pausanias_df['tokens'] = pausanias_df['unannotated_strings'].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenization process through SpaCy adds some features to each of the tokens in the `tokens` column. Now we can collect the types and exclude stop words using the `token.is_stop` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Counter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m types \u001b[38;5;241m=\u001b[39m [t\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m pausanias_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokens\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mexplode() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_stop \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_alpha]\n\u001b[0;32m----> 3\u001b[0m type_counts \u001b[38;5;241m=\u001b[39m \u001b[43mCounter\u001b[49m(types)\n\u001b[1;32m      5\u001b[0m type_counts\u001b[38;5;241m.\u001b[39mmost_common(\u001b[38;5;241m100\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Counter' is not defined"
     ]
    }
   ],
   "source": [
    "types = [t.text for t in pausanias_df['tokens'].explode() if not t.is_stop and t.is_alpha]\n",
    "\n",
    "type_counts = Counter(types)\n",
    "\n",
    "type_counts.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! We now have a list of the most common types, exluding stop words and punctuation.\n",
    "\n",
    "Be careful, though: these are still just raw counts, and they tell us very little about how we might characterize Pausanias vis-à-vis a larger corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmata/lemmas\n",
    "\n",
    "A **lemma** (plural **lemmata** or **lemmas**) represents \"a group of all inflectional forms related to one stem that belong to the same word class (Kučera & Francis 1967: 1)\" [@Brezina2018 40]. In simpler terms, a **lemma** is the dictionary form of a word, so **lemmata** give us a way of further reducing the word count. ἐστίν, ἔσμεν, and εἰσίν all have the same **lemma**: εἰμί.\n",
    "\n",
    "Lemmatization, as you might guess, often involves additional processing. Luckily, we can use the SpaCy and GreCy models again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pletcher/code/classes/quant-text-analysis/.venv/lib/python3.11/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
     ]
    }
   ],
   "source": [
    "raw_texts = [t for t in pausanias_df['unannotated_strings']]\n",
    "annotated_texts = nlp.pipe(raw_texts, batch_size=100)\n",
    "\n",
    "pausanias_df['nlp_docs'] = list(annotated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ποιέω', 1201),\n",
       " ('λέγω', 1185),\n",
       " ('ἔχω', 1152),\n",
       " ('σφεῖς', 1093),\n",
       " ('γίγνομαι', 1090),\n",
       " ('παῖς', 827),\n",
       " ('καλέω', 797),\n",
       " ('πόλις', 778),\n",
       " ('φημί', 765),\n",
       " ('πολύς', 742),\n",
       " ('ἄγαλμα', 697),\n",
       " ('ἀνήρ', 678),\n",
       " ('ἱερόν', 657),\n",
       " ('θεός', 644),\n",
       " ('Λακεδαιμόνιος', 574),\n",
       " ('ἐνταῦθα', 525),\n",
       " ('λόγος', 482),\n",
       " ('πᾶς', 473),\n",
       " ('ἐπί', 454),\n",
       " ('μάλα', 449),\n",
       " ('ναός', 445),\n",
       " ('ὄνομα', 434),\n",
       " ('γυνή', 393),\n",
       " ('Ἀθηναῖος', 383),\n",
       " ('ὅσος', 380),\n",
       " ('ὕστερος', 376),\n",
       " ('γῆ', 371),\n",
       " ('Μεσσήνιος', 370),\n",
       " ('μέγας', 369),\n",
       " ('Ἀπόλλων', 359),\n",
       " ('Ἕλλην', 358),\n",
       " ('ὀνομάζω', 351),\n",
       " ('πρῶτος', 350),\n",
       " ('ἀρχή', 345),\n",
       " ('ἀφικνέομαι', 337),\n",
       " ('Ζεύς', 327),\n",
       " ('στάδιον', 327),\n",
       " ('Ἠλεῖος', 321),\n",
       " ('ποταμός', 320),\n",
       " ('ἄγω', 319),\n",
       " ('ἄνθρωπος', 316),\n",
       " ('ἔργον', 311),\n",
       " ('ἀρχαῖος', 308),\n",
       " ('πρότερος', 306),\n",
       " ('πόλεμος', 303),\n",
       " ('Ἀχαιός', 298),\n",
       " ('τότε', 291),\n",
       " ('θάλασσα', 288),\n",
       " ('Ἀργεῖος', 285),\n",
       " ('μάχη', 281),\n",
       " ('ὄρος', 273),\n",
       " ('Ἀρκάς', 272),\n",
       " ('λίθος', 272),\n",
       " ('ὕδωρ', 267),\n",
       " ('ἕτερος', 265),\n",
       " ('Ἄρτεμις', 262),\n",
       " ('χώρα', 257),\n",
       " ('ἤδη', 257),\n",
       " ('βασιλεύς', 256),\n",
       " ('θυγάτηρ', 248),\n",
       " ('ἵππος', 244),\n",
       " ('βωμός', 243),\n",
       " ('δέ', 234),\n",
       " ('Ἀθήνη', 232),\n",
       " ('ἀνατίθημι', 232),\n",
       " ('ἀποθνῄσκω', 231),\n",
       " ('ὁδός', 228),\n",
       " ('οἶδα', 225),\n",
       " ('νίκη', 223),\n",
       " ('ἔρχομαι', 222),\n",
       " ('μνῆμα', 217),\n",
       " ('δίδωμι', 216),\n",
       " ('καθίστημι', 216),\n",
       " ('νομίζω', 216),\n",
       " ('δοκέω', 214),\n",
       " ('παρέχω', 212),\n",
       " ('ἐκεῖνος', 211),\n",
       " ('χρόνος', 209),\n",
       " ('Θηβαῖος', 208),\n",
       " ('εἰκών', 206),\n",
       " ('Δελφοί', 206),\n",
       " ('Ὀλυμπία', 205),\n",
       " ('λαμβάνω', 202),\n",
       " ('ὁράω', 201),\n",
       " ('Ἡρακλῆς', 197),\n",
       " ('οἰκέω', 197),\n",
       " ('ἔτος', 196),\n",
       " ('βασιλεύω', 195),\n",
       " ('Ῥωμαῖος', 195),\n",
       " ('αὖθις', 188),\n",
       " ('ὁπόσος', 186),\n",
       " ('γράφω', 184),\n",
       " ('ἀπό', 182),\n",
       " ('τίθημι', 181),\n",
       " ('ἐπίκλησις', 181),\n",
       " ('αἱρέω', 177),\n",
       " ('Ποσειδῶν', 177),\n",
       " ('ἵστημι', 174),\n",
       " ('χάλκεος', 174),\n",
       " ('Διόνυσος', 174)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T# #############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexemes\n",
    "\n",
    "Finally, \"a **lexeme** is a lemma with a particular meaning attached to it.... The best way of conceptualizing a lexeme is as a subentry in a dictionary\" [@Brezina2018 40].\n",
    "\n",
    "One challenge of working with lexemes is that, even with the advances of large language models like ChatGPT, there is no surefire way to annotate them automatically. We still need \"human-in-the-loop\" pipelines to catch errors and ambiguities. And keep in mind that even two humans might disagree on the lexeme for a particular word!\n",
    "\n",
    "But we can inspect the `lex` attributes of the tokens that SpaCy has generated for us and see if they make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('ἐνταῦθα', <spacy.lexeme.Lexeme at 0x39f333c80>), 491),\n",
       " (('μάλιστα', <spacy.lexeme.Lexeme at 0x39f333980>), 421),\n",
       " (('ἄγαλμα', <spacy.lexeme.Lexeme at 0x392a7dbc0>), 411),\n",
       " (('ἱερὸν', <spacy.lexeme.Lexeme at 0x392abfe40>), 369),\n",
       " (('ὕστερον', <spacy.lexeme.Lexeme at 0x392a7f840>), 351),\n",
       " (('σφισιν', <spacy.lexeme.Lexeme at 0x39f332b00>), 347),\n",
       " (('ὄνομα', <spacy.lexeme.Lexeme at 0x392a89180>), 345),\n",
       " (('γενέσθαι', <spacy.lexeme.Lexeme at 0x392a88300>), 329),\n",
       " (('λέγουσιν', <spacy.lexeme.Lexeme at 0x392a7e240>), 328),\n",
       " (('τότε', <spacy.lexeme.Lexeme at 0x392a80980>), 287),\n",
       " (('ἐπʼ', <spacy.lexeme.Lexeme at 0x392a851c0>), 268),\n",
       " (('πόλιν', <spacy.lexeme.Lexeme at 0x392a7e3c0>), 258),\n",
       " (('ἤδη', <spacy.lexeme.Lexeme at 0x392aa1b00>), 257),\n",
       " (('σφᾶς', <spacy.lexeme.Lexeme at 0x392a92680>), 250),\n",
       " (('φασιν', <spacy.lexeme.Lexeme at 0x392a7d500>), 238),\n",
       " (('δʼ', <spacy.lexeme.Lexeme at 0x392ac2a80>), 234),\n",
       " (('πρότερον', <spacy.lexeme.Lexeme at 0x39f331d80>), 227),\n",
       " (('σφίσιν', <spacy.lexeme.Lexeme at 0x39f3322c0>), 216),\n",
       " (('παῖδα', <spacy.lexeme.Lexeme at 0x392a8a500>), 215),\n",
       " (('ἐγένετο', <spacy.lexeme.Lexeme at 0x392aa4cc0>), 203),\n",
       " (('πεποίηται', <spacy.lexeme.Lexeme at 0x392a7e700>), 199),\n",
       " (('ὕδωρ', <spacy.lexeme.Lexeme at 0x392a95e40>), 196),\n",
       " (('λέγουσι', <spacy.lexeme.Lexeme at 0x392a9a180>), 194),\n",
       " (('Λακεδαιμονίων', <spacy.lexeme.Lexeme at 0x392a7c840>), 190),\n",
       " (('ἐφʼ', <spacy.lexeme.Lexeme at 0x392a83a80>), 186),\n",
       " (('ἔχει', <spacy.lexeme.Lexeme at 0x39f333b40>), 184),\n",
       " (('αὖθις', <spacy.lexeme.Lexeme at 0x39f333f00>), 183),\n",
       " (('ἱερόν', <spacy.lexeme.Lexeme at 0x392a7c7c0>), 183),\n",
       " (('λίθου', <spacy.lexeme.Lexeme at 0x392b27300>), 183),\n",
       " (('Ἀθηνᾶς', <spacy.lexeme.Lexeme at 0x39f33d640>), 178),\n",
       " (('θεῶν', <spacy.lexeme.Lexeme at 0x392a7d240>), 176),\n",
       " (('Ἀθηναίων', <spacy.lexeme.Lexeme at 0x392a85040>), 168),\n",
       " (('λέγεται', <spacy.lexeme.Lexeme at 0x392a8d1c0>), 167),\n",
       " (('Ἀπόλλωνος', <spacy.lexeme.Lexeme at 0x392b14400>), 166),\n",
       " (('ἀγάλματα', <spacy.lexeme.Lexeme at 0x39f333b00>), 164),\n",
       " (('Ἑλλήνων', <spacy.lexeme.Lexeme at 0x392a93300>), 164),\n",
       " (('σφισι', <spacy.lexeme.Lexeme at 0x392a91d00>), 162),\n",
       " (('δύο', <spacy.lexeme.Lexeme at 0x392ab8e00>), 162),\n",
       " (('πρὸ', <spacy.lexeme.Lexeme at 0x392a844c0>), 160),\n",
       " (('φασὶν', <spacy.lexeme.Lexeme at 0x39f332bc0>), 159),\n",
       " (('ἔνθα', <spacy.lexeme.Lexeme at 0x39f330bc0>), 158),\n",
       " (('ὁμοῦ', <spacy.lexeme.Lexeme at 0x392a8ffc0>), 154),\n",
       " (('ἅτε', <spacy.lexeme.Lexeme at 0x392a93500>), 154),\n",
       " (('ἀρχῆς', <spacy.lexeme.Lexeme at 0x392a7fc40>), 153),\n",
       " (('Διὸς', <spacy.lexeme.Lexeme at 0x39f333a00>), 151),\n",
       " (('Ἀθηναίοις', <spacy.lexeme.Lexeme at 0x39f331800>), 149),\n",
       " (('Ἀχαιῶν', <spacy.lexeme.Lexeme at 0x3f2d4b080>), 146),\n",
       " (('πολὺ', <spacy.lexeme.Lexeme at 0x392aac240>), 142),\n",
       " (('Ἀρτέμιδος', <spacy.lexeme.Lexeme at 0x392a7cec0>), 139),\n",
       " (('ἔργον', <spacy.lexeme.Lexeme at 0x392a7c680>), 138),\n",
       " (('ἀρχαῖον', <spacy.lexeme.Lexeme at 0x392a92700>), 138),\n",
       " (('ἐπίκλησιν', <spacy.lexeme.Lexeme at 0x392a8f840>), 135),\n",
       " (('ἀπʼ', <spacy.lexeme.Lexeme at 0x392a9b200>), 134),\n",
       " (('γῆν', <spacy.lexeme.Lexeme at 0x392a9b800>), 134),\n",
       " (('Λακεδαιμόνιοι', <spacy.lexeme.Lexeme at 0x3e3827100>), 134),\n",
       " (('παίδων', <spacy.lexeme.Lexeme at 0x392a7d340>), 132),\n",
       " (('Ἡρακλέους', <spacy.lexeme.Lexeme at 0x392a9d1c0>), 131),\n",
       " (('Λακεδαιμονίοις', <spacy.lexeme.Lexeme at 0x392a8e4c0>), 130),\n",
       " (('ἐποίησεν', <spacy.lexeme.Lexeme at 0x392a82c00>), 129),\n",
       " (('μνῆμα', <spacy.lexeme.Lexeme at 0x392a7e440>), 128),\n",
       " (('πάντα', <spacy.lexeme.Lexeme at 0x392a8e6c0>), 128),\n",
       " (('χωρίον', <spacy.lexeme.Lexeme at 0x392a7ed40>), 127),\n",
       " (('ἕνεκα', <spacy.lexeme.Lexeme at 0x392aa0600>), 127),\n",
       " (('πλησίον', <spacy.lexeme.Lexeme at 0x392a83200>), 126),\n",
       " (('γῆς', <spacy.lexeme.Lexeme at 0x3d5dd06c0>), 124),\n",
       " (('θεοῦ', <spacy.lexeme.Lexeme at 0x392a7cb00>), 124),\n",
       " (('πόλεως', <spacy.lexeme.Lexeme at 0x39f332a80>), 122),\n",
       " (('παρʼ', <spacy.lexeme.Lexeme at 0x392a8ecc0>), 122),\n",
       " (('ὅσον', <spacy.lexeme.Lexeme at 0x392a92c00>), 121),\n",
       " (('ναὸς', <spacy.lexeme.Lexeme at 0x39f33d500>), 120),\n",
       " (('Ἀρκάδων', <spacy.lexeme.Lexeme at 0x3e39668c0>), 117),\n",
       " (('λόγῳ', <spacy.lexeme.Lexeme at 0x392a854c0>), 116),\n",
       " (('διʼ', <spacy.lexeme.Lexeme at 0x392a93b00>), 116),\n",
       " (('παῖδας', <spacy.lexeme.Lexeme at 0x392a7c180>), 115),\n",
       " (('ἄνδρα', <spacy.lexeme.Lexeme at 0x392aacd40>), 115),\n",
       " (('Ὀλυμπίᾳ', <spacy.lexeme.Lexeme at 0x3e38e02c0>), 115),\n",
       " (('πρὶν', <spacy.lexeme.Lexeme at 0x39f3328c0>), 114),\n",
       " (('λόγος', <spacy.lexeme.Lexeme at 0x392a847c0>), 114),\n",
       " (('καλούμενον', <spacy.lexeme.Lexeme at 0x392a84cc0>), 114),\n",
       " (('ναός', <spacy.lexeme.Lexeme at 0x392a7d140>), 112),\n",
       " (('χρόνον', <spacy.lexeme.Lexeme at 0x392a83100>), 112),\n",
       " (('ἀνθρώπων', <spacy.lexeme.Lexeme at 0x392a88e80>), 112),\n",
       " (('ὄρος', <spacy.lexeme.Lexeme at 0x392a99f00>), 112),\n",
       " (('ὅσοι', <spacy.lexeme.Lexeme at 0x392ab7a40>), 112),\n",
       " (('πρῶτον', <spacy.lexeme.Lexeme at 0x392a87280>), 111),\n",
       " (('ὁπόσα', <spacy.lexeme.Lexeme at 0x392a8d4c0>), 111),\n",
       " (('ἀνὴρ', <spacy.lexeme.Lexeme at 0x392ab47c0>), 109),\n",
       " (('σταδίους', <spacy.lexeme.Lexeme at 0x392a7d880>), 108),\n",
       " (('πόλει', <spacy.lexeme.Lexeme at 0x392adfd00>), 108),\n",
       " (('θάλασσαν', <spacy.lexeme.Lexeme at 0x392a89f80>), 107),\n",
       " (('αὐτίκα', <spacy.lexeme.Lexeme at 0x392aa6d40>), 107),\n",
       " (('πόλις', <spacy.lexeme.Lexeme at 0x3e37d5bc0>), 107),\n",
       " (('νῦν', <spacy.lexeme.Lexeme at 0x392a7e1c0>), 106),\n",
       " (('δεξιᾷ', <spacy.lexeme.Lexeme at 0x392a89580>), 106),\n",
       " (('θεῷ', <spacy.lexeme.Lexeme at 0x392a8fcc0>), 106),\n",
       " (('γένος', <spacy.lexeme.Lexeme at 0x392ac1900>), 106),\n",
       " (('πλέον', <spacy.lexeme.Lexeme at 0x392ac54c0>), 106),\n",
       " (('μάχῃ', <spacy.lexeme.Lexeme at 0x39f333ec0>), 103),\n",
       " (('Μεσσηνίων', <spacy.lexeme.Lexeme at 0x392b0c4c0>), 103),\n",
       " (('Ἠλείων', <spacy.lexeme.Lexeme at 0x3fc8b9580>), 103)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexemes = [(t.text, t.lex) for t in pausanias_df['nlp_docs'].explode() if not t.is_stop and t.is_alpha]\n",
    "\n",
    "lexeme_counts = Counter(lexemes)\n",
    "\n",
    "lexeme_counts.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait a second -- this list looks identical to our list of word types.\n",
    "\n",
    "Sure enough, when we check the SpaCy documentation for [Lexeme](https://spacy.io/api/lexeme):\n",
    "\n",
    "> A Lexeme has no string context – it’s a word type, as opposed to a word token. It therefore has no part-of-speech tag, dependency parse, or lemma (if lemmatization depends on the part-of-speech tag)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Review\n",
    "\n",
    "> Discuss: Define **token**, **type**, **stop word**, **lemma**, and **lexeme** in your own words.\n",
    "\n",
    "> Discuss: How can we use these different notions of \"word\" in our analysis of corpora? Why is it important to be precise about what kind of word(s) we're using?\n",
    "\n",
    "## Homework\n",
    "\n",
    "1. Read @Brezina2018 [ch. 2, pp. 41--65].\n",
    "2. Choose 3 books of Pausanias and calculate the most common tokens, types, and lemmata for each. In a paragraph or so, describe your findings relative to the work we have done in class today.\n",
    "3. Using your findings from 2., write a short (1-page) evaluation of one of the books of Pausanias that you have analyzed. Does your qualitative -- which is not to say \"subjective\" -- experience of reading the text cohere with your quantitative evaluation?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
