{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "\n",
    "## Reporting: Different Kinds of Frequencies\n",
    "\n",
    "When reporting your findings from last week, you've mainly been using \"absolute frequency\" (AF). There are, however, many ways report word frequencies in a corpus. As we go over these frequencies, consider the trade-offs and advantages of each.\n",
    "\n",
    "### Absolute (\"raw\") frequency\n",
    "\n",
    "Brezina defines AF as \"a count of all tokens in the text or corpus that belong to a particular word type\" [@Brezina2018 42]. He uses the example of the 6,041,234 occurrences of the token \"the\" in the British National Corpus (BNC). Since Greek inflects the definite article, we can't simply count the occurrences of a single token.\n",
    "\n",
    "> Discuss: When should you use absolute frequency in reporting?\n",
    "\n",
    "As usual, let's load up our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MyCapytain.resources.texts.local.capitains.cts import CapitainsCtsText\n",
    "\n",
    "with open(\"../tei/tlg0525.tlg001.perseus-grc2.xml\") as f:\n",
    "    text = CapitainsCtsText(urn=\"urn:cts:greekLit:tlg0525.tlg001.perseus-grc2\", resource=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's count the occurrences of the definite article across the whole work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "from MyCapytain.common.constants import Mimetypes\n",
    "\n",
    "urns = []\n",
    "raw_xmls = []\n",
    "unannotated_strings = []\n",
    "\n",
    "for ref in text.getReffs(level=len(text.citation)):\n",
    "    urn = f\"{text.urn}:{ref}\"\n",
    "    node = text.getTextualNode(ref)\n",
    "    raw_xml = node.export(Mimetypes.XML.TEI)\n",
    "    tree = node.export(Mimetypes.PYTHON.ETREE)\n",
    "    s = etree.tostring(tree, encoding=\"unicode\", method=\"text\")\n",
    "\n",
    "    urns.append(urn)\n",
    "    raw_xmls.append(raw_xml)\n",
    "    unannotated_strings.append(s)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "d = {\n",
    "    \"urn\": pd.Series(urns, dtype=\"string\"),\n",
    "    \"raw_xml\": raw_xmls,\n",
    "    \"unannotated_strings\": pd.Series(unannotated_strings, dtype=\"string\")\n",
    "}\n",
    "pausanias_df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will take a while\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"grc_proiel_sm\", disable=[\"ner\"])\n",
    "\n",
    "raw_texts = [t for t in pausanias_df['unannotated_strings']]\n",
    "annotated_texts = nlp.pipe(raw_texts, batch_size=100)\n",
    "\n",
    "pausanias_df['nlp_docs'] = list(annotated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definite_article = [t for t in pausanias_df['nlp_docs'].explode() if t.lemma_ == \"ὁ\"]\n",
    "\n",
    "len(definite_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus we have 30,698 occurrences of the definite article in Pausanias.\n",
    "\n",
    "When might we want to use absolute frequency? As we've already started to see when working with Pausanias, absolute frequency can be useful for sorting tokens and lemmata in a single corpus. When comparing multiple corpora, however, absolute frequency does not provide a good comparison: consider the problem when comparing a relatively small corpus, like Aeschylus' seven extant plays, to all of Pausanias: absolute frequencies as a point of comparison would be essentially meaningless. Or to take a more ready-to-hand example, what would it mean to compare the frequencies in a single book of Pausanias to the entire work? For this kind of analysis, we need to use **relative frequency**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative (\"normalized\") frequency\n",
    "\n",
    "Relative frequency [@Brezina2018 43] is easy to calculate: take the absolute frequency of the target word and divide it by the total words in the corpus. By convention, we then multiply it by a constant, the \"basis for normalization\" -- this converts the measurement from a percentage of tokens in the corpus to an expression of tokens per thousand -- or million, whatever you decide to make your constant.\n",
    "\n",
    "To practice, let's calculate the number of definite articles per million words in Pausanias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_definite_article = len(definite_article)\n",
    "n_tokens = len([t for t in pausanias_df['nlp_docs'].explode()])\n",
    "basis = 1_000_000\n",
    "\n",
    "rf_definite_article_in_pausanias = (n_definite_article / n_tokens) * basis\n",
    "\n",
    "print(rf_definite_article_in_pausanias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you've probably surmised from looking at this calculation, the relative frequency \"can ... be considered as ... the mean of the frequencies of the word in hypothetical samples of _x_ tokens from the corpus, where _x_ is the basis for normalization\" [@Brezina2018 43].\n",
    "\n",
    "In other words, if we were to divide Pausanias into equal 1-million-word chunks, counting the occurrences of the definite article in each chunk and then averaging them, we would arrive at the same number.\n",
    "\n",
    "Keep this intuition in mind for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hapax legomena (\"once-saids\")\n",
    "\n",
    "_Hapax legomena_ (singular _hapax legomenon_ or simply _hapax_) are words that occur only once in the corpus. We can get a sense of hapaxes in Pausanias by counting the occurrences of lemmata like we did last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Including the integer .lemma property so that we can verify whether Spacy is assigning different lemmata\n",
    "# to tokens that should be the same\n",
    "counts = Counter([(t.lemma, t.lemma_) for t in pausanias_df['nlp_docs'].explode() if t.is_alpha])\n",
    "hapaxes = [h for (h, i) in counts.items() if i == 1]\n",
    "\n",
    "hapaxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Discuss: Do you notice any potential issues in the list of hapaxes above? How should we report them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipf's Law\n",
    "\n",
    "Using the `counts` that we calculated above, we can examine Zipf's law in the context of Pausanias. Zipf's law, to borrow Brezina's summary, \"tells us that when we start with the most frequent item in the wordlist (regardless of the size of the corpus), the second most frequent item will have only half of the frequency of the first item. The third most common word will have one-third of the frequency of the first item; and so on\" [@Brezina2018 44]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.most_common(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratios aren't exact, but they're pretty close! Zipf's law might be easier to see if we visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"altair[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "top_50_words = counts.most_common(50)\n",
    "\n",
    "top_50_words.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "zipfs_df = pd.DataFrame([{'lemma': h, 'frequency': i, 'rank': idx} for idx, (h, i) in enumerate(top_50_words)])\n",
    "\n",
    "chart = alt.Chart(zipfs_df)\n",
    "\n",
    "chart.mark_point().encode(x='rank', y='frequency')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the rapid decrease in frequencies maps nicely on the Zipf's law visualization at @Brezina2018 [45].\n",
    "\n",
    "> In-class exercise: We can see the visualization well enough here, but how can we improve this chart? Use the [Altair documentation](https://altair-viz.github.io/index.html) to make the chart more readable. As a bonus, see if you can figure out how to show the lemma for each rank when you hover over the point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dispersion\n",
    "\n",
    "> Discuss: In your own words, describe the so-called \"whelk problem\" [@Brezina2018 46--47]. Who coined the phrase, and why?\n",
    "\n",
    "> Generally, dispersion tells us about the distribution of words or phrases throughout the corpus. For example, the definite article the is not only a highly frequent word, it also is fairly evenly distributed in text. This is because the is a grammatical word and we usually cannot put sentences together without using it. Other words which are specific to a particular context (e.g. whelk, hashtag, corpus) will be less evenly distributed. [@Brezina2018 47]\n",
    "\n",
    "### Range<sub>2</sub> (R)\n",
    "\n",
    "Range<sub>2</sub> tells us in how many parts of a corpus a given word appears, regardless of the size of each part. It can also be expressed as a percentage, e.g., if a word appears in 8 out of 10 books of Pausanias, it would have an _R_ value of 8/10 or 80%.\n",
    "\n",
    "> In-class exercise: Divide the quotation above into sentences and determine the R<sub>2</sub> dispersion of forms of the word \"be\". Bonus: do the same, but divide the quotation into five-word chunks (you should have one one-word chunk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: typically you will want to use a lemmatizer, but as we are working with sample data, we can \n",
    "# just create a set of forms to work with here\n",
    "forms_of_be = set([\"be\", \"am\", \"are\", \"is\", \"was\", \"were\", \"been\"]) \n",
    "\n",
    "quotation = \"\"\"\n",
    "Generally, dispersion tells us about the distribution of words or phrases throughout the corpus. \n",
    "For example, the definite article the is not only a highly frequent word, it also is fairly evenly distributed in text. \n",
    "This is because the is a grammatical word and we usually cannot put sentences together without using it. \n",
    "Other words which are specific to a particular context (e.g. whelk, hashtag, corpus) will be less evenly distributed.\n",
    "\"\"\"\n",
    "\n",
    "# Your code below. Helpful built-in functions: str.split(), str.splitlines().\n",
    "quotation_lines = [l.split() for l in quotation.splitlines() if l.strip() != \"\"]\n",
    "quotation_r_2 = [1 for l in quotation_lines if len(set(l).intersection(forms_of_be)) > 0]\n",
    "\n",
    "print(sum(quotation_r_2) / len(quotation_lines))\n",
    "\n",
    "## Bonus\n",
    "\n",
    "quotation_chunks = [quotation.split()[i:i+5] for i in range(0, len(quotation.split()), 5)]\n",
    "\n",
    "print(quotation_chunks)\n",
    "\n",
    "quotation_chunks_r_2 = [1 for c in quotation_chunks if len(set(c).intersection(forms_of_be)) > 0]\n",
    "\n",
    "print(sum(quotation_chunks_r_2) / len(quotation_chunks))\n",
    "\n",
    "# Things to notice: the size of each part is arbitrary\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Deviation\n",
    "\n",
    "The **standard deviation** (σ) attempts to answer how much variation around the mean occurs in the data. You've probably seen this measurement of dispersion outside of corpus linguistics, but here it can show, for example, variation in the relative frequency of a word in different parts of a corpus.\n",
    "\n",
    "Standard deviation is expressed mathematically as `sqrt(sum of squared distances from the mean / total # of corpus parts)`.'\n",
    "\n",
    "> Discuss: Why do we square the differences from the mean if we're also going to take the square root of the ratio?\n",
    "\n",
    "#### Sample standard deviation\n",
    "\n",
    "Sample standard deviation differs from standard deviation (σ) only in the divisor, which is here `total # of corpus parts - 1`.\n",
    "\n",
    "> In-class exercise: calculate the standard deviation and the sample standard deviation of the relative frequency of forms of \"be\" in the quotation from Brezina."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "rel_freqs_be = []\n",
    "\n",
    "for line in quotation_lines:\n",
    "    n_be = len([t for t in line if t in forms_of_be])\n",
    "    rel_freqs_be.append((n_be * 10) / len(line))\n",
    "\n",
    "print(rel_freqs_be)\n",
    "\n",
    "average_be_per_10_tokens = sum(rel_freqs_be) / len(rel_freqs_be)\n",
    "\n",
    "print(average_be_per_10_tokens)\n",
    "\n",
    "import math\n",
    "\n",
    "def std_dev(samples):\n",
    "    mean = sum(samples) / len(samples)\n",
    "\n",
    "    return math.sqrt(sum([(n - mean)**2 for n in samples])) / len(samples)\n",
    "\n",
    "def sample_std_dev(samples):\n",
    "    mean = sum(samples) / len(samples)\n",
    "\n",
    "    return math.sqrt(sum([(n - mean)**2 for n in samples])) / (len(samples) - 1)\n",
    "\n",
    "std_dev_be = std_dev(rel_freqs_be)\n",
    "\n",
    "sample_std_dev_be = sample_std_dev(rel_freqs_be)\n",
    "\n",
    "print(f\"σ = {std_dev_be} | SD = {sample_std_dev_be}\")\n",
    "\n",
    "# Note that the variations here are pretty wide -- we should expect this from such a small data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficient of Variation (CV)\n",
    "\n",
    "\"The coefficient of variation (CV) describes the amount of variation relative to the mean relative frequency of a word or phrase in the corpus\" [@Brezina2018 50].\n",
    "\n",
    "We calculate the coefficient of variation by dividing the standard deviation by the mean: `CV = std. deviation / mean`.\n",
    "\n",
    "> In-class exercise: What is the coefficient of variation for forms of the word \"be\" in the quotation from Brezina when dividing by sentence?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "cv_brezina_quotation = std_dev_be / (sum(rel_freqs_be) / len(rel_freqs_be))\n",
    "\n",
    "print(cv_brezina_quotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Juilland's _D_\n",
    "\n",
    "> Juilland’s _D_ is a measure of dispersion that builds on the coefficient of variation. It is a number between 0 and 1, with 0 signifying extremely uneven distribution and 1 perfectly even distribution. [@Brezina2018 51]\n",
    "\n",
    "You can think of Juilland's _D_ as the inverse of the coefficient of variation: \"While CV tells us about the amount of variation in the corpus (larger CV means more variation in the frequencies), Juilland’s D tells us about homogeneity of the distribution (larger Juilland’s D means a more even distribution and less variation)\" [@Brezina2018 51].\n",
    "\n",
    "Juilland's _D_ is calculated by `CV / sqrt(# corpus parts - 1)`.\n",
    "\n",
    "> In-class exercise: What is the Juilland's _D_ for forms of the word \"be\" in the quotation from Brezina (split into sentences)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "j_d_brezina_quotation = cv_brezina_quotation / math.sqrt(len(quotation_lines) - 1)\n",
    "\n",
    "print(j_d_brezina_quotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deviation of Proportions (DP)\n",
    "\n",
    "**Deviation of Proportions** is similar to Juilland's _D_ insofar as it measures dispersion in a corpus; but it uses the reverse scale, where 0 indicates perfectly even dispersion and 1 indicates an extremely uneven distribution.\n",
    "\n",
    "It is calculated by taking the `sum(| observed - expected proportions |) / 2`.\n",
    "\n",
    "The `expected proportions` are calculated by dividing the sizes of each corpus part (in tokens) divided by the total size of the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "brezina_sents = [l.strip().split() for l in quotation.splitlines() if l != \"\"]\n",
    "total_brezina_tokens = sum(len(s) for s in brezina_sents)\n",
    "expected_proportions = [len(s) / total_brezina_tokens for s in brezina_sents] \n",
    "expected_proportions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `observed proportions` are then calculated by taking the absolute frequency of a word or phrase of interest in each part divided by the absolute frequency of the word or phrase in the whole corpus.\n",
    "\n",
    "Calculate the DP for the lemma \"be\" in the Brezina quotation below, using the `expected_proportions` provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "abs_freq_corpus = sum([1 for t in quotation.split() if t in forms_of_be])\n",
    "observed_proportions_by_line = []\n",
    "\n",
    "for line in quotation_lines:\n",
    "    n = 0\n",
    "    for t in line:\n",
    "        if t in forms_of_be:\n",
    "            n += 1\n",
    "    observed_proportions_by_line.append(n / abs_freq_corpus)\n",
    "\n",
    "print(abs_freq_corpus)\n",
    "print(observed_proportions_by_line)\n",
    "\n",
    "dp_brezina_quotation = sum([abs(e - o) for (e, o) in zip(expected_proportions, observed_proportions_by_line)]) / 2\n",
    "\n",
    "print(dp_brezina_quotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Reduced Frequency (ARF)\n",
    "\n",
    "The key idea behind **Average Reduced Frequency** is that we can discard occurrences of a word that are close together to get a better picture of the word's significance to the corpus as a whole.\n",
    "\n",
    "**ARF** is calculated as follows (in pseudo-code):\n",
    "\n",
    "```\n",
    "w = word\n",
    "v = total corpus tokens / absolute frequency of w\n",
    "\n",
    "ARF = 1/v * (min(distance_1, v) + min(distance_2, v) + ... min(distance_n, v))\n",
    "```\n",
    "\n",
    "If we think of the corpus as a circle rather than a line, we can imagine repeating the `min(distance, v)` procedure for every occurrence of `w`, wrapping around the text at the end.\n",
    "\n",
    "ARF, in other words, is a \"reduction\" of the word's frequency based on the dispersion of its occurrences throughout the corpus. [@Brezina2018 53--57]\n",
    "\n",
    "> Discuss: How can ARF be used to address the whelk problem mentioned above?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quotation_tokens = [t for t in quotation.split() if t != \"\"]\n",
    "locations_of_be = [idx if t in forms_of_be else 0 for (idx, t) in enumerate(quotation_tokens)]\n",
    "\n",
    "def arf(locations):\n",
    "    points = [i for i in locations if i != 0]\n",
    "    v = len(locations) / len(points)\n",
    "\n",
    "    distances = [min(points[0] + len(locations) - points[-1], v)] + [min(p, v) for p in points[1:]]\n",
    "\n",
    "    return (1/v) * sum(distances)\n",
    "\n",
    "arf(locations_of_be)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Lexical Diversity\n",
    "\n",
    "Lexical diversity helps us measure whether a corpus uses a wide or limited range of vocabulary.\n",
    "\n",
    "### Type/Token Ratio (TTR)\n",
    "\n",
    "One of the simplest ways to calculate lexical diversity is the **type/token ration (TTR)**. This calculation is driven by the intuition that a corpus with a relatively high number of word forms (types) compared to total words (tokens) exhibits a wider range of expression than a corpus of the same size with a lower number of types.\n",
    "\n",
    "```\n",
    "TTR = no. types / no. tokens\n",
    "```\n",
    "\n",
    "> In-class exercise: Calculate the TTR for the Brezina quotation. You will need to use the SpaCy lemmatizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "eng_nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(quotation)\n",
    "tokens = [t for t in doc if t.is_alpha]\n",
    "types = set([t.text for t in tokens])\n",
    "\n",
    "brezina_ttr = len(types) / len(tokens)\n",
    "\n",
    "brezina_ttr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Discuss: What problems emerge from this simple TTR calculation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardized Type/Token Ratio (STTR)\n",
    "\n",
    "As its name implies, **Standardized Type/Token Ratio (STTR)** divides the text into standardized chunks of, e.g., 1000 tokens, discarding the last chunk. It then calculates the TTR for each chunk and reports the mean of all chunks' TTRs.\n",
    "\n",
    "### Moving Average Type/Token Ratio (MATTR)\n",
    "\n",
    "> Hint: See https://docs.python.org/3/library/itertools.html, especially the `sliding_windows()` recipe.\n",
    "\n",
    "Similarly, **Moving Average Type/Token Ratio (MATTR)** calculates the mean of multiple TTRs as a _moving average_ (i.e., an overlapping window) of chunks through the corpus.\n",
    "\n",
    "> Discuss: How does Brezina's \"transformation of Zipf's law to express rank\" [-@Brezina2018 60] work? How can we use it for Pausanias?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "1. Find a word that appears no more than 20 times in all of Pausanias, and calculate it's R<sub>2</sub> for Pausanias' text when divided by book.\n",
    "2. Find the standard deviation of the relative frequencies of forms of ποιέω in the books of Pausanias.\n",
    "3. Calculate the deviation of proportions for a word of your choosing in the books of Pausanias.\n",
    "4. Calculate the TTR for each book of Pausanias, and for Pausanias as a whole.\n",
    "5. Calculate the MATTR of Pausanias using a sliding window of 5000 tokens."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
