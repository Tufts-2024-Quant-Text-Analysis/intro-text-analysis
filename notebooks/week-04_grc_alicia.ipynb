{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantics and Discourse\n",
    "\n",
    "(Brezina 2018: ch. 3, pp. 66–75)\n",
    "\n",
    "## Collocations\n",
    "\n",
    "Definitions (@Brezina2018, 67): \n",
    "\n",
    "- **Collocation**: a group of two or more words \"that habitually co-occur in texts and corpora.\"\n",
    "- **Collocation measures**: \"statistical meausres that calculate the strength of association between words based on different aspects of the co-occurrence relationship.\"\n",
    "- **Node**: \"word that we want to search for and analyse.\"\n",
    "- **Collocates**: \"words that co-occur with the node in a specifically defined **span** around the node, which we call the **collocation window**.\"\n",
    "- **Observed frequency of collocation**: Number of times that a **collocate** appears with a **node**.\n",
    "\n",
    "## The simple approach\n",
    "\n",
    "> Discuss: Why might one avoid a basic ranked list of collocates?\n",
    "\n",
    "\n",
    "## A more sophisticated approach\n",
    "\n",
    "- **Expected frequency of collocation**\n",
    "\n",
    "```python\n",
    "expected_collocate_freq = (node_freq * collocate_freq * window_size) / n_tokens_in_corpus\n",
    "```\n",
    "\n",
    "> Discuss: Explain Brezina's example of \"my\" and \"love\" in Robert Burns' \"A Red, Red Rose.\"\n",
    "\n",
    "> Discuss: What problems does one encounter if one uses this approach blindly?\n",
    "\n",
    "\n",
    "## Association Measures\n",
    "\n",
    "Let's prepare to explore these collocation measures by loading up a dataframe of Pausanias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've created a utils.py file for frequently reused functionality -- you can import from it like so\n",
    "from utils import load_pausanias\n",
    "\n",
    "pausanias_df = load_pausanias()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calculating co-occurrences in Greek, it is generally insufficient to use the L and R windows that Brezina uses for English (@Brezina2018 67–70). Instead, we'll look for a dependency relationship between the **node** and its **collocates**. Below, you can see that we can access the dependencies of a token through its `children` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token = pausanias_df['nlp_docs'][0][1]\n",
    "\n",
    "# we use a list comprehension to evaluate the generator at `test_token.children`\n",
    "f\"token: '{test_token}, {test_token.lemma_}', dependencies: {[(c, c.lemma_) for c in test_token.children]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we're also accessing the `lemma_` property here. Because Greek is heavily inflected, we'll tend to focus on collocations of lemmata, rather than types -- but you might find in your own work that it is interesting to look at type collocations instead. Just be sure to note which kind of \"word\" you're examining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of co-occurrence\n",
    "\n",
    "The frequency of co-occurrence reports the presence of both a **node** (`w1`) and a **collocate** (`w2`). Given a dataframe like `pausanias_df`, we can calculate the frequency of co-occurrence using something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 'ἄγαλμα'\n",
    "w2 = 'εἰμί'\n",
    "\n",
    "def count_collocations(x, w1, w2):\n",
    "    w2_is_child_of_w1 = len([t for t in x if t.lemma_ == w1 and w2 in [tt.lemma_ for tt in t.children]])\n",
    "    w1_is_child_of_w2 = len([t for t in x if t.lemma_ == w2 and w1 in [tt.lemma_ for tt in t.children]])\n",
    "\n",
    "    return w2_is_child_of_w1 + w1_is_child_of_w2\n",
    "\n",
    "pausanias_df['agalma_eimi_collocations'] = pausanias_df['nlp_docs'].apply(count_collocations, args=(w1, w2))\n",
    "\n",
    "pausanias_df[pausanias_df['agalma_eimi_collocations'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Mutual Uninformation (MU)\n",
    "\n",
    "The ratio of the **observed** frequency (O11) and **expected** frequency (E11).\n",
    "\n",
    "If the ratio is greater than 1, the words co-appear more frequently than expected.\n",
    "\n",
    "### Mutual Information (MI)\n",
    "\n",
    "`log<sub>2</sub>(O11/E11)`\n",
    "\n",
    "#### MI2\n",
    "\n",
    "\n",
    "#### MI3\n",
    "\n",
    "\n",
    "### Log-likelihood (LL)\n",
    "\n",
    "\n",
    "### Z-score<sub>1</sub>\n",
    "\n",
    "\n",
    "### T-score\n",
    "\n",
    "\n",
    "### Dice\n",
    "\n",
    "\n",
    "### Log Dice\n",
    "\n",
    "\n",
    "### Log ratio\n",
    "\n",
    "\n",
    "### Minimum Sensitivity (MS)\n",
    "\n",
    "\n",
    "### Delta P\n",
    "\n",
    "\n",
    "### Cohen's *d*\n",
    "\n",
    "\n",
    "## Directionality and Dispersion"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
