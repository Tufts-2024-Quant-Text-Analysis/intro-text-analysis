{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantics and Discourse\n",
    "\n",
    "(Brezina 2018: ch. 3, pp. 66–75)\n",
    "\n",
    "## Collocations\n",
    "\n",
    "Definitions (@Brezina2018, 67): \n",
    "\n",
    "- **Collocation**: a group of two or more words \"that habitually co-occur in texts and corpora.\"\n",
    "- **Collocation measures**: \"statistical meausres that calculate the strength of association between words based on different aspects of the co-occurrence relationship.\"\n",
    "- **Node**: \"word that we want to search for and analyse.\"\n",
    "- **Collocates**: \"words that co-occur with the node in a specifically defined **span** around the node, which we call the **collocation window**.\"\n",
    "- **Observed frequency of collocation**: Number of times that a **collocate** appears with a **node**.\n",
    "\n",
    "## The simple approach\n",
    "\n",
    "> Discuss: Why might one avoid a basic ranked list of collocates?\n",
    "\n",
    "\n",
    "## A more sophisticated approach\n",
    "\n",
    "- **Expected frequency of collocation**\n",
    "\n",
    "```python\n",
    "expected_collocate_freq = (node_freq * collocate_freq * window_size) / n_tokens_in_corpus\n",
    "```\n",
    "\n",
    "> Discuss: Explain Brezina's example of \"my\" and \"love\" in Robert Burns' \"A Red, Red Rose.\"\n",
    "\n",
    "> Discuss: What problems does one encounter if one uses this approach blindly?\n",
    "\n",
    "\n",
    "## Association Measures\n",
    "\n",
    "Let's prepare to explore these collocation measures by loading up a dataframe of Pausanias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I've created a utils.py file for frequently reused functionality -- you can import from it like so\n",
    "from utils import load_pausanias\n",
    "\n",
    "pausanias_df = load_pausanias('eng') # you can use `load_pausanias('eng')` to load the English version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calculating co-occurrences in Greek, it is generally insufficient to use the L and R windows that Brezina uses for English (@Brezina2018 67–70). Instead, we'll look for a dependency relationship between the **node** and its **collocates**. Below, you can see that we can access the dependencies of a token through its `children` property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_token = pausanias_df['nlp_docs'][0][1]\n",
    "\n",
    "# we use a list comprehension to evaluate the generator at `test_token.children`\n",
    "f\"token: '{test_token}, {test_token.lemma_}', dependencies: {[(c, c.lemma_) for c in test_token.children]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we're also accessing the `lemma_` property here. Because Greek is heavily inflected, we'll tend to focus on collocations of lemmata, rather than types -- but you might find in your own work that it is interesting to look at type collocations instead. Just be sure to note which kind of \"word\" you're examining."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency of co-occurrence\n",
    "\n",
    "The frequency of co-occurrence reports the presence of both a **node** (`w1`) and a **collocate** (`w2`). Given a DataFrame like `pausanias_df`, we can calculate the frequency of co-occurrence in two different ways. \n",
    "\n",
    "We can either count when the collocate is a dependency of the node, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = 'statue'\n",
    "collocate = 'the'\n",
    "\n",
    "def count_dependency_collocations(x, w1, w2):\n",
    "    w2_is_child_of_w1 = len([t for t in x if t.lemma_ == w1 and w2 in [tt.lemma_ for tt in t.children]])\n",
    "\n",
    "    return w2_is_child_of_w1\n",
    "\n",
    "pausanias_df['agalma_megas_dependencies'] = pausanias_df['nlp_docs'].apply(count_dependency_collocations, args=(node, collocate))\n",
    "\n",
    "observed_dep_freq_agalma_megas = pausanias_df[pausanias_df['agalma_megas_dependencies'] > 0].shape[0]\n",
    "\n",
    "observed_dep_freq_agalma_megas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can count when the collocate and node co-occur within a given window, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngram_collocations(x, w1, w2, l_size: int = 1, r_size: int = 1):\n",
    "    lemmata = [t.lemma_ for t in x]\n",
    "\n",
    "    indexes = [i for i, lemma in enumerate(lemmata) if lemma == w1]\n",
    "\n",
    "    cooccurrences = 0\n",
    "\n",
    "    for i in indexes:\n",
    "        left = max(i - l_size, 0)\n",
    "        right = min(i + r_size + 1, len(lemmata))\n",
    "\n",
    "        window = lemmata[left:right]\n",
    "\n",
    "        if w2 in window:\n",
    "            cooccurrences += 1\n",
    "            \n",
    "    return cooccurrences\n",
    "\n",
    "pausanias_df['agalma_megas_1l-1r'] = pausanias_df['nlp_docs'].apply(count_ngram_collocations, args=(node, collocate))\n",
    "\n",
    "observed_1l_1r_freq_agalma_megas = pausanias_df[pausanias_df['agalma_megas_1l-1r'] > 0].shape[0]\n",
    "\n",
    "observed_1l_1r_freq_agalma_megas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that a 1L, 1R window detects 17 collocations of ἄγαλμα and μέγας, 4 *more* than we detected as dependencies. Remember, these collocations aren't necessarily related grammatically anymore, but it's interesting to see how the count changes. Let's try with a larger window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pausanias_df['agalma_megas_2l-2r'] = pausanias_df['nlp_docs'].apply(count_ngram_collocations, args=(node, collocate, 2, 2))\n",
    "\n",
    "observed_2l_2r_freq_agalma_megas = pausanias_df[pausanias_df['agalma_megas_2l-2r'] > 0].shape[0]\n",
    "\n",
    "observed_2l_2r_freq_agalma_megas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can experiment in your own notebooks by adjusting the `l_size` and `r_size` args passed to the `count_ngram_collocations` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evert's μ (Mu)\n",
    "\n",
    "(Brezina never, as far as I can tell, defines this term, and confusingly refers to it with capital letters. I'm still trying to figure out why he does this.)\n",
    "\n",
    "Stephanie<sup>*</sup> @Evert2005 [54] defines μ as follows:\n",
    "\n",
    "> For μ > 1 we speak of positive association (where the components are more likely to occur together than if they were independent), and for μ < 1 we speak of negative association (where the components are less likely to occur together than if they were independent).\n",
    "\n",
    "She adds the following in a note:\n",
    "\n",
    "> The letter μ is intended to be reminiscent of _mutual information_, since the quantity log(μ) can be interpreted as point-wise mutual information. I have avoided using this term for μ, though, so as not to confuse information theory with population parameters.\n",
    "\n",
    "In other words, μ says that f the ratio is greater than 1, the words co-appear more frequently than expected.\n",
    "\n",
    "We calculate μ by taking the ratio of the **observed** frequency (represented by O11 in a contingency table) and **expected** frequency (E11).\n",
    "\n",
    "\n",
    "<sup>*</sup> Note that you might find references to Stephanie Evert's work (including the 2005 doctoral thesis cited above) under her former name, Stefan Evert.\n",
    "\n",
    "#### Observed Frequency\n",
    "\n",
    "As discussed above, we're calculating observed frequency of collocation by looking at dependency trees. This is a somewhat more complicated procedure than simply looking to the left and right of a word, and we'll need to account for it when we calculate the **random co-occurrence baseline**, the result of which is the **expected frequency of collocation**.\n",
    "\n",
    "#### Expected Frequency \n",
    "\n",
    "Expected frequency is calculated by taking the frequency of the **node** in the entire corpus times the frequency of the **collocate**, divided by the number of tokens in the corpus.\n",
    "\n",
    "Without any corrections, this method assumes that the tokens co-occur right next to each other (either before or after, but not both). To correct for the greater probability of the tokens co-occurring when we are not looking at immediate adjacency, we mutliply the numerator in this equation by the **window size**.\n",
    "\n",
    "_However_, the use of syntactic dependency trees obviates this correction: window size would mean something like the number of adjacent treebanks to check for a collocation, which would be linguistically meaningless.\n",
    "\n",
    "Window size is thus a correction for doing n-gram–based analyses of collocations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def expected_frequency_of_collocation(df: pd.DataFrame, node: str, collocate: str, window_size: int = 1):\n",
    "    \"\"\"\n",
    "    `node` and `collocate` should be the string representations\n",
    "    of the associated lemmata\n",
    "    \"\"\"\n",
    "\n",
    "    lemmata = [t.lemma_ for t in df['nlp_docs'].explode()]\n",
    "    counter = Counter(lemmata)\n",
    "    node_count = counter[node]\n",
    "    collocate_count = counter[collocate]\n",
    "\n",
    "    return (node_count * collocate_count * window_size) / len(lemmata)\n",
    "\n",
    "\n",
    "expected_freq_agalma_megas = expected_frequency_of_collocation(pausanias_df, node, collocate)\n",
    "\n",
    "mu_deps = observed_dep_freq_agalma_megas / expected_freq_agalma_megas\n",
    "mu_1l_1r = observed_1l_1r_freq_agalma_megas / expected_freq_agalma_megas\n",
    "\n",
    "print(f\"μ for statue with dependency the: {mu_deps}\\n\\nμ for statue and the in a 1L, 1R window: {mu_1l_1r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information (MI)\n",
    "\n",
    "Mutual information measures how much the appearance of one word in our collocate pair suggests the appearance of the other word. We can calculate it by taking the log<sub>2</sub> of `mu` (observed / expected frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "mutual_information_agalma_megas_deps = math.log(mu_deps, 2)\n",
    "\n",
    "mutual_information_agalma_megas_deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_information_agalma_megas_1l_1r = math.log(mu_1l_1r, 2)\n",
    "mutual_information_agalma_megas_1l_1r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A value greater than 1 indicates that the presence of the node -- in this demo, statue -- implies the presence of the collocate -- here, the.\n",
    "\n",
    "Does the same hold true in the other direction, that is, when statue is a dependent of the?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pausanias_df['megas_agalma_collocations'] = pausanias_df['nlp_docs'].apply(count_dependency_collocations, args=(collocate, node))\n",
    "\n",
    "observed_freq_megas_agalma = pausanias_df[pausanias_df['megas_agalma_collocations'] > 0].shape[0]\n",
    "\n",
    "## Note that the expected frequency does not change depending on which direction the dependency goes\n",
    "mu = observed_freq_megas_agalma / expected_freq_agalma_megas\n",
    "\n",
    "mutual_information_megas_agalma = math.log(mu, 2)\n",
    "\n",
    "mutual_information_megas_agalma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we find that the appearance of μέγας slightly implies the non-appearance of ἄγαλμα among its dependencies. This makes sense: μέγας is an adjective, and we wouldn't expect it to govern a noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other association measures covered by Brezina tend to be variations on this theme, using mostly the same inputs with adjustments to the weights to make the calculation more or less sensitive to the collocate pair's **exclusivity**.\n",
    "\n",
    "> Discuss: Define \"exclusivity\" in the context of collocations.\n",
    "\n",
    "\n",
    "## Directionality and Dispersion\n",
    "\n",
    "If we want to measure the **directionality** of the association, however, we need to use a calculation like **Delta P**, which reports two statistics: one for the predictability of the node with respect to the collocate, and one for the predictability of the collocate to the node.\n",
    "\n",
    "### Delta P\n",
    "\n",
    "Translated into English, Delta P looks for:\n",
    "\n",
    "- The observed frequency of the collocate pair in the corpus (O11), divided by the frequency of the node in the corpus (R1)\n",
    "  - minus the observed frequency of _the collocate **without** the node_ in the corpus (O21), divided by the tokens that are not the node in the corpus (R2)\n",
    "\n",
    "AND\n",
    "\n",
    "- The observed frequency of the collocate pair (O11), divided by the frequency of the *collocate* in the corpus (C1)\n",
    "  - minus the observed frequency of _the node **without** the collocate_ (O12), divided by the tokens that are not the collocate (C2)\n",
    "\n",
    "Notice that it does not take expected frequencies into account.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = \"temple\"\n",
    "collocate = \"a\"\n",
    "\n",
    "def count_ngram_collocations(x, w1, w2, l_size: int = 1, r_size: int = 1):\n",
    "    lemmata = [t.lemma_ for t in x]\n",
    "\n",
    "    indexes = [i for i, lemma in enumerate(lemmata) if lemma == w1]\n",
    "\n",
    "    cooccurrences = 0\n",
    "\n",
    "    for i in indexes:\n",
    "        left = max(i - l_size, 0)\n",
    "        right = min(i + r_size + 1, len(lemmata))\n",
    "        window = lemmata[left:right]\n",
    "\n",
    "        if w2 in window:\n",
    "            cooccurrences += 1\n",
    "            \n",
    "    return cooccurrences\n",
    "\n",
    "pausanias_df['o11_temple_a_ngrams'] = pausanias_df['nlp_docs'].apply(count_ngram_collocations, args=(node, collocate))\n",
    "\n",
    "o11 = pausanias_df['o11_temple_a_ngrams'].sum()\n",
    "\n",
    "all_tokens = pausanias_df['nlp_docs'].explode()\n",
    "\n",
    "r1 = len([t for t in all_tokens if t.lemma_ == node])\n",
    "r2 = len(all_tokens) - r1\n",
    "c1 = len([t for t in all_tokens if t.lemma_ == collocate])\n",
    "o21 = c1 - o11\n",
    "\n",
    "(o11 / r1) - (o21 / r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Dispersion\n",
    "\n",
    "Dispersion can be measured in the pretty much the same way as in Section 2.4 of @Brezina2018: \n",
    "\n",
    "1. Divide the corpus into chunks (size doesn't matter as long as you normalize by corpus length). \n",
    "2. Calculate the expected proportions: (# tokens in chunk / window size) / (# tokens in corpus / window size)\n",
    "3. Calculate observed proportions of collocation: (# of cols. in chunk) / (# of cols. in corpus)\n",
    "\n",
    "From here, you have all the information you need to calculate the Deviation of Proportions (DP).\n",
    "\n",
    "You can also calculate Cohen's *d*:\n",
    "\n",
    "- Let X be the mean of the frequencies of the collocation in each chunk\n",
    "- Let Y be the mean of the frequencies in each chunk where either the node or collocate is absent\n",
    "  - i.e., Y = the mean of the frequencies of the node or collocate minus the number of collocations in a given chunk\n",
    "- Let S<sub>X</sub> be the standard deviation of the frequencies used in X\n",
    "- Let S<sub>Y</sub> be the standard deviation of the frequencies used in Y\n",
    "\n",
    "\n",
    "d = (X - Y) / sqrt((S<sub>X</sub><sup>2</sup> + S<sub>Y</sub><sup>2</sup>) / 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "You may work in groups for these exercises. They are due at the beginning of next class. You can submit them as a link to a Colab notebook or GitHub CodeSpace.\n",
    "\n",
    "0. Use the corpora that you assembled last week (Pausanias++):\n",
    "1. Using programming techniques from the course so far, find other potential collocates for a word of your choice.\n",
    "2. Calculate the μ and Mutual Information scores for at least 5 of these collocate pairs. How do your results change depending on your definition of a collocation? What might these changes mean? (Write your answers to these questions down.)\n",
    "3. Calculate the Delta P for these same five pairs. Do any results stand out? Why? What might they tell us about your corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translated into English, Delta P looks for:\n",
    "\n",
    "- The observed frequency of the collocate pair in the corpus (O11), divided by the frequency of the node in the corpus (R1)\n",
    "  - minus the observed frequency of _the collocate **without** the node_ in the corpus (O21), divided by the tokens that are not the node in the corpus (R2)\n",
    "\n",
    "AND\n",
    "\n",
    "- The observed frequency of the collocate pair (O11), divided by the frequency of the *collocate* in the corpus (C1)\n",
    "  - minus the observed frequency of _the node **without** the collocate_ (O12), divided by the tokens that are not the collocate (C2)\n",
    "\n",
    "Notice that it does not take expected frequencies into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_pausanias\n",
    "\n",
    "df = load_pausanias('eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = \"mainland\"\n",
    "collocate = \"the\"\n",
    "\n",
    "\n",
    "def count_ngram_collocations(x, w1, w2, l_size: int = 1, r_size: int = 1):\n",
    "    lemmata = [t.lemma_ for t in x]\n",
    "\n",
    "    indexes = [i for i, lemma in enumerate(lemmata) if lemma == w1]\n",
    "\n",
    "    cooccurrences = 0\n",
    "\n",
    "    for i in indexes:\n",
    "        left = max(i - l_size, 0)\n",
    "        right = min(i + r_size + 1, len(lemmata))\n",
    "        window = lemmata[left:right]\n",
    "\n",
    "        if w2 in window:\n",
    "            cooccurrences += 1\n",
    "\n",
    "    return cooccurrences\n",
    "\n",
    "\n",
    "def count_ngram_non_collocations(x, w1, w2, l_size: int = 1, r_size: int = 1):\n",
    "    lemmata = [t.lemma_ for t in x]\n",
    "\n",
    "    indexes = [i for i, lemma in enumerate(lemmata) if lemma == w1]\n",
    "\n",
    "    non_cooccurrences = 0\n",
    "\n",
    "    for i in indexes:\n",
    "        left = max(i - l_size, 0)\n",
    "        right = min(i + r_size + 1, len(lemmata))\n",
    "        window = lemmata[left:right]\n",
    "\n",
    "        if w2 not in window:\n",
    "            non_cooccurrences += 1\n",
    "\n",
    "    return non_cooccurrences\n",
    "\n",
    "\n",
    "r1 = len([t for t in df[\"nlp_docs\"].explode() if t.lemma_ == node])\n",
    "r2 = len([t for t in df[\"nlp_docs\"].explode() if t.lemma_ != node])\n",
    "c1 = len([t for t in df[\"nlp_docs\"].explode() if t.lemma_ == collocate])\n",
    "c2 = len([t for t in df[\"nlp_docs\"].explode() if t.lemma_ != collocate])\n",
    "\n",
    "o11 = df[\"nlp_docs\"].apply(count_ngram_collocations, args=(node, collocate)).sum()\n",
    "o12 = df[\"nlp_docs\"].apply(count_ngram_non_collocations, args=(node, collocate)).sum()\n",
    "# o12 = r1 - o11\n",
    "o21 = c1 - o11\n",
    "\n",
    "print(f\"r1: {r1}, r2: {r2}, c1: {c1}, c2: {c2}, o11: {o11}, o12: {o12}, o21: {o21}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(o11 / r1) - (o21 / r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(o11 / c1) - (o12 / c2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
